# Language-Guided Socially Aware Embodied Navigation

Achieving socially aware navigation remains an ongoing challenge in robotics. Social robot navigation is a leap toward seamlessly integrating embodied agents into dynamic human environments. Despite recent advancements, existing methodologies fail to adequately address a wide span of human social conventions while navigating dense environments. Building upon the foundation laid by NaviSTAR and the MuSoHu dataset, this paper introduces a novel approach to social robot navigation using language-guided methods.

Our methodology is driven by a multi-modal model, implementing Visual-Language Models (VLMs) to decode high-level text commands and subtle social cues from human interactions. Unlike conventional navigation systems, our approach embraces the complexity of human-environment interactions by leveraging a diverse array of sensors and datasets. %In particular, we augment our training data with the SCAND dataset, enriching our model's understanding of socially compliant navigation behaviors.

Central to our methodology is the adept use of VLMs, which enables our system to decode intricate social cues and high-level textual commands, thereby elevating the robots' understanding of and responsiveness to their social surroundings. This is complemented by the system's adaptability to a wide range of user preferences and navigation styles, achieved through instruction tuning mechanisms. Such adaptability ensures that robotic actions are more closely aligned with human expectations, improving the quality of interaction. Our work lays a solid foundation for the future development of navigation systems that enable robots to navigate and interact fluently in the nuanced and ever-changing landscape of human social settings.
